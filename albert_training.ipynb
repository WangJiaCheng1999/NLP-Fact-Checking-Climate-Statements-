{"cells":[{"cell_type":"code","source":["!pip install SentencePiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1c8LhnQF6MFF","executionInfo":{"status":"ok","timestamp":1684131864617,"user_tz":-600,"elapsed":4669,"user":{"displayName":"Jiacheng Wang","userId":"08806092138054227226"}},"outputId":"055e7bfe-92b2-4bd1-dd66-ee301a4dbbd1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: SentencePiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcfC6A2F6Qap","executionInfo":{"status":"ok","timestamp":1684131868623,"user_tz":-600,"elapsed":4007,"user":{"displayName":"Jiacheng Wang","userId":"08806092138054227226"}},"outputId":"c60c93c0-ff17-412e-a5bd-297b8d0cbb58"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UvIqBnGkyeZQ","executionInfo":{"status":"ok","timestamp":1684131872458,"user_tz":-600,"elapsed":3838,"user":{"displayName":"Jiacheng Wang","userId":"08806092138054227226"}},"outputId":"1d7fc82e-def0-40fa-8a19-2d0e8e262442"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import transformers\n","from transformers import AlbertTokenizer, AlbertModel\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from gensim.models import KeyedVectors\n","from transformers import AlbertTokenizer, AlbertModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","label_names = {'SUPPORTS': 0, 'NOT_ENOUGH_INFO': 1, 'REFUTES': 2, 'DISPUTED': 3}\n","evd = pd.read_json('/content/drive/MyDrive/Colab Notebooks/NLP project/FactChecker_NLP/data/evidence.json', orient='index')\n","evidences_list = evd.iloc[:, 0].tolist()\n","org_train = pd.read_json('/content/drive/MyDrive/Colab Notebooks/NLP project/FactChecker_NLP/data/train-claims.json').T\n","org_dev = pd.read_json('/content/drive/MyDrive/Colab Notebooks/NLP project/FactChecker_NLP/data/dev-claims.json').T\n","\n","#prep the data needed for evidence label classification\n","def bert_classifier_prep(train_claims):\n","    bert_train = []\n","    #loop every row of the train_claims\n","    for row in train_claims.iterrows():\n","        label = label_names[row[1]['claim_label']]\n","        if label == 3:\n","            continue\n","        else:\n","            claim_text = row[1]['claim_text']\n","\n","            for evidence_id in row[1]['evidences']:\n","                evidence_id = int(evidence_id.split('-')[1])\n","                evidence_text = evidences_list[evidence_id]\n","                bert_train.append((claim_text, label, evidence_text))\n","            \n","    return bert_train\n","\n","\n","class TrainDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=120):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        claim_text, claim_label, evidence_text = self.data[idx]\n","        inputs = self.tokenizer(\n","            claim_text,\n","            evidence_text,\n","            return_attention_mask=True,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        token_ids = inputs['input_ids'][0]\n","        mask = inputs['attention_mask'][0]\n","        \n","        return {'input_ids':token_ids, 'attention_mask':mask, 'target':torch.tensor(claim_label, dtype=torch.long)}\n","\n","class EvidenceClassifier(torch.nn.Module):\n","    def __init__(self):\n","        super(EvidenceClassifier, self).__init__()\n","        self.bert = transformers.AlbertModel.from_pretrained('albert-base-v2')\n","        self.dropout1 = torch.nn.Dropout(p=0.2)  # increased dropout\n","        self.fc1 = torch.nn.Linear(self.bert.config.hidden_size, 32)  # decreased model complexity\n","        self.dropout2 = torch.nn.Dropout(p=0.2)  # add another dropout layer\n","        self.fc2 = torch.nn.Linear(32, 3)  # final fully connected layer\n","        \n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout1(pooled_output)\n","        fc1_output = self.fc1(pooled_output)\n","        fc1_output = self.dropout2(fc1_output)\n","        logits = self.fc2(fc1_output)\n","        return logits\n","\n","\n","def evaluate(model, dataloader, criterion):\n","    model.eval()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(dataloader):\n","            input_ids = batch['input_ids'].cuda()\n","            attention_mask = batch['attention_mask'].cuda()\n","            target = batch['target'].cuda()\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","            loss = criterion(outputs, target)\n","\n","            running_loss += loss.item() * input_ids.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            running_corrects += torch.sum(preds == target)\n","\n","    epoch_loss = running_loss / len(dataloader.dataset)\n","    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n","\n","    return epoch_loss, epoch_acc\n","\n","def train(model, dataloader, criterion, optimizer):\n","    model.train()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    label_names = [0,1,2,3]\n","    label_corrects = [0] * len(label_names)\n","    label_totals = [0] * len(label_names)\n","    for i, batch in enumerate(dataloader):\n","        input_ids = batch['input_ids'].cuda()\n","        attention_mask = batch['attention_mask'].cuda()\n","        target = batch['target'].cuda()\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        loss = criterion(outputs, target)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item() * input_ids.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        running_corrects += torch.sum(preds == target)\n","        for i in range(len(label_names)):\n","          label_corrects[i] += torch.sum((preds == i) & (target == i))\n","          label_totals[i] += torch.sum(target == i)\n","    test_accs = [correct.item() / total.item() if total.item() != 0 else 0 for correct, total in zip(label_corrects, label_totals)]\n","    epoch_loss = running_loss / len(dataloader.dataset)\n","    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n","    for label, acc in zip(label_names, test_accs):\n","      print(f'Test Acc ({label}): {acc:.4f}')\n","    return epoch_loss, epoch_acc\n","\n","\n","def pretrain_bert(early_stop_epochs=2):\n","    tokenizer = transformers.AlbertTokenizer.from_pretrained('albert-base-v2')\n","    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","    model = EvidenceClassifier().cuda()\n","    weight_decay = 1e-5\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=weight_decay)\n","\n","    best_loss = None\n","    no_improve_epochs = 0\n","\n","    for epoch in range(8):\n","        train_loss, train_acc = train(model, train_dataloader, criterion, optimizer)\n","        test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n","\n","        if best_loss is None or test_loss < best_loss:\n","            best_loss = test_loss\n","            no_improve_epochs = 0\n","            # save model\n","            torch.save(model.state_dict(), 'albert_classifier.pt')\n","        else:\n","            no_improve_epochs += 1\n","\n","        print(f'Epoch: {epoch+1} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}')\n","\n","        if no_improve_epochs >= early_stop_epochs:\n","            print(\"Early stopping due to no improvement in validation loss for {} epochs.\".format(early_stop_epochs))\n","            break\n","\n","\n","tokenizer = transformers.AlbertTokenizer.from_pretrained('albert-base-v2')\n","train_dataset = TrainDataset(bert_classifier_prep(org_train), tokenizer, 300)\n","test_dataset = TrainDataset(bert_classifier_prep(org_dev), tokenizer, 300)\n","pretrain_bert()"],"metadata":{"id":"EM4kAThSxx_l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684132082697,"user_tz":-600,"elapsed":210241,"user":{"displayName":"Jiacheng Wang","userId":"08806092138054227226"}},"outputId":"9a23411a-9440-43a7-ca9e-afd918e61fc2"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Test Acc (0): 0.3135\n","Test Acc (1): 0.7886\n","Test Acc (2): 0.0263\n","Test Acc (3): 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | Train Loss: 0.9517 | Train Acc: 0.5241 | Test Loss: 0.9345 | Test Acc: 0.5843\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Test Acc (0): 0.7327\n","Test Acc (1): 0.8601\n","Test Acc (2): 0.0000\n","Test Acc (3): 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2 | Train Loss: 0.7533 | Train Acc: 0.7088 | Test Loss: 0.9200 | Test Acc: 0.5982\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Test Acc (0): 0.8995\n","Test Acc (1): 0.9487\n","Test Acc (2): 0.0044\n","Test Acc (3): 0.0000\n","Epoch: 3 | Train Loss: 0.5328 | Train Acc: 0.8153 | Test Loss: 0.9888 | Test Acc: 0.6097\n"]},{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"]},{"output_type":"stream","name":"stdout","text":["Test Acc (0): 0.9635\n","Test Acc (1): 0.9860\n","Test Acc (2): 0.1247\n","Test Acc (3): 0.0000\n","Epoch: 4 | Train Loss: 0.3709 | Train Acc: 0.8724 | Test Loss: 1.1103 | Test Acc: 0.5866\n","Early stopping due to no improvement in validation loss for 2 epochs.\n"]}]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}