{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","mount_file_id":"1MTaQkfKazKV_P-5UHB2yZ5OKZNSF755_","authorship_tag":"ABX9TyPao+nseSgmmzWE6Opq0XKY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install sentencepiece\n","!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YpRI9KWIfmcB","executionInfo":{"status":"ok","timestamp":1683948388025,"user_tz":-600,"elapsed":13381,"user":{"displayName":"Jiacheng Wang","userId":"08806092138054227226"}},"outputId":"b44da4ff-3b74-4eba-81ef-3d236c4bb7c0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"]}]},{"cell_type":"code","source":["from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n","import numpy as np\n","import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","from tqdm import tqdm\n","import json\n","\n","encoded_evidences = np.load(\"/content/drive/MyDrive/Colab Notebooks/NLP project/FactChecker_NLP/data/dpr_evidence_embeddings.npy\", allow_pickle=True)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"z78fgG6QJga4","executionInfo":{"status":"ok","timestamp":1683948453566,"user_tz":-600,"elapsed":4513,"user":{"displayName":"Jiacheng Wang","userId":"08806092138054227226"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G5zANNxKfiI4","executionInfo":{"status":"ok","timestamp":1683951774597,"user_tz":-600,"elapsed":2780881,"user":{"displayName":"Jiacheng Wang","userId":"08806092138054227226"}},"outputId":"ec6f9a4d-71e2-4cc8-dbe4-1a839650f716"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 153/153 [46:18<00:00, 18.16s/it]\n"]}],"source":["model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/NLP project/FactChecker_NLP/data/models/bert_model.pt\"))\n","model = model.to(device)\n","\n","def predict_claim_category_and_evidences(claim, model, encoded_evidences):\n","    # 初始化DPR问题编码器和tokenizer\n","    question_encoder_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n","    question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n","\n","    # 编码claim\n","    encoded_claim = question_encoder_tokenizer(claim, return_tensors=\"pt\")\n","    claim_embedding = question_encoder_model(**encoded_claim).pooler_output.detach().numpy()\n","\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","\n","    # 计算与所有证据的相似度\n","    similarities = []\n","    for encoded_evidence in encoded_evidences:\n","        similarity = np.inner(claim_embedding, encoded_evidence[\"embedding\"])\n","        similarities.append((encoded_evidence[\"id\"], similarity))\n","\n","    # 对相似度进行排序，取前6个最相关的证据\n","    top_evidences = sorted(similarities, key=lambda x: x[1], reverse=True)[:5]\n","    # 使用分类器预测claim的类别\n","    encoding = tokenizer(claim, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","    input_ids = encoding['input_ids'].to(device)\n","    attention_mask = encoding['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n","\n","\n","    return predicted_label, [evidence_id for evidence_id, _ in top_evidences]\n","\n","dev = pd.read_json('/content/drive/MyDrive/Colab Notebooks/NLP project/FactChecker_NLP/data/dev-claims.json').T\n","test = pd.read_json('/content/drive/MyDrive/Colab Notebooks/NLP project/FactChecker_NLP/data/test-claims-unlabelled.json').T\n","\n","predictions = {}\n","LABELS = [\"SUPPORTS\", \"REFUTES\", \"DISPUTED\", \"NOT_ENOUGH_INFO\"]\n","# 对dev数据集中的每个claim进行预测\n","for index, row in tqdm(test.iterrows(), total=test.shape[0]):\n","    claim = row['claim_text']\n","    label, evidences = predict_claim_category_and_evidences(claim, model, encoded_evidences)\n","    evidence_ids = [f\"evidence-{id}\" for id in evidences]\n","    predictions[index] = {\"claim_text\": claim, \"claim_label\": LABELS[label], \"evidences\": evidence_ids}\n","\n","# 将预测结果保存到json文件中\n","with open('predictions.json', 'w') as f:\n","    json.dump(predictions, f)\n","\n"]}]}